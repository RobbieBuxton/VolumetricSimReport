@inproceedings{10.1145/1411204.1411255,
  author    = {Dolstra, Eelco and L\"{o}h, Andres},
  title     = {NixOS: A Purely Functional Linux Distribution},
  year      = {2008},
  isbn      = {9781595939197},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1411204.1411255},
  doi       = {10.1145/1411204.1411255},
  abstract  = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as upgrading packages or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to run multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogueously to a heap in a purely function language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a purely functional specification.},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
  pages     = {367–378},
  numpages  = {12},
  keywords  = {package management, nix, purely functional language, purely functional deployment model, software deployment, NixOS, system configuration management},
  location  = {Victoria, BC, Canada},
  series    = {ICFP '08}
}

@article{10.1145/1411203.1411255,
  author     = {Dolstra, Eelco and L\"{o}h, Andres},
  title      = {NixOS: A Purely Functional Linux Distribution},
  year       = {2008},
  issue_date = {September 2008},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {43},
  number     = {9},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1411203.1411255},
  doi        = {10.1145/1411203.1411255},
  abstract   = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as upgrading packages or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to run multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogueously to a heap in a purely function language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a purely functional specification.},
  journal    = {SIGPLAN Not.},
  month      = {sep},
  pages      = {367–378},
  numpages   = {12},
  keywords   = {purely functional deployment model, software deployment, package management, purely functional language, nix, system configuration management, NixOS}
}

@inproceedings{dolstra2004nix,
  title     = {Nix: A Safe and Policy-Free System for Software Deployment.},
  author    = {Dolstra, Eelco and De Jonge, Merijn and Visser, Eelco and others},
  booktitle = {LISA},
  volume    = {4},
  pages     = {79--92},
  year      = {2004}
}

@book{dolstra2006purely,
  title     = {The purely functional software deployment model},
  author    = {Dolstra, Eelco},
  year      = {2006},
  publisher = {Utrecht University}
}

@misc{Marakasov_2024,
  url     = {https://repology.org/repositories/graphs},
  journal = {Graphs - Repology},
  author  = { Marakasov, Dmitry},
  year    = {2024},
  month   = {01},
  urldate = {2024-01-09}
} 

@online{nixcon-sandboxs,
  title        = {About Nix sandboxes and breakpoints (NixCon 2018)},
  year         = {2018},
  organization = {Youtube},
  author       = {Jörg Thalheim},
  url          = {https://www.youtube.com/watch?v=ULqoCjANK-I},
  urldate      = {2024-01-09}
}

@article{LHCbNix,
  author  = {{Burr, Chris} and {Clemencic, Marco} and {Couturier, Ben}},
  title   = {Software packaging and distribution for LHCb using Nix},
  doi     = {10.1051/epjconf/201921405005},
  url     = {https://doi.org/10.1051/epjconf/201921405005},
  journal = {EPJ Web Conf.},
  year    = 2019,
  volume  = 214,
  pages   = {05005}
}

@inproceedings{10.1145/3152493.3152556,
  author    = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  title     = {Nix as HPC Package Management System},
  year      = {2017},
  isbn      = {9781450351300},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3152493.3152556},
  doi       = {10.1145/3152493.3152556},
  abstract  = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialised hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialised package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  booktitle = {Proceedings of the Fourth International Workshop on HPC User Support Tools},
  articleno = {4},
  numpages  = {6},
  keywords  = {Nix, Package Management System, High Performance Computing},
  location  = {Denver, CO, USA},
  series    = {HUST'17}
}

@article{https://doi.org/10.1002/qua.26872,
  author   = {Kowalewski, Markus and Seeber, Phillip},
  title    = {Sustainable packaging of quantum chemistry software with the Nix package manager},
  journal  = {International Journal of Quantum Chemistry},
  volume   = {122},
  number   = {9},
  pages    = {e26872},
  keywords = {high-performance computing, quantum chemistry, reproducible environments, Nix package manager, software development},
  doi      = {https://doi.org/10.1002/qua.26872},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qua.26872},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/qua.26872},
  abstract = {Abstract The installation of quantum chemistry software packages is commonly done manually and can be a time-consuming and complicated process. An update of the underlying Linux system requires a reinstallation in many cases and can quietly break software installed on the system. In this paper, we present an approach that allows for an easy installation of quantum chemistry software packages, which is also independent of operating system updates. The use of the Nix package manager allows building software in a reproducible manner, which allows for a reconstruction of the software for later reproduction of scientific results. The build recipes that are provided can be readily used by anyone to avoid complex installation procedures.},
  year     = {2022}
}

@misc{NixCommunityNixOSWiki,
  url     = {https://nixos.wiki/wiki/Nix_Community},
  journal = {Nix Community - NixOS Wiki},
  urldate = {2024-01-09}
} 

@misc{NixPkgs,
  url     = {https://github.com/NixOS/nixpkgs},
  journal = {Nixpkgs, a collection of over 80,000 software package},
  urldate = {2024-01-09},
  author  = {NixOS Foundation}
}
@article{kooima2009generalized,
  title  = {Generalized perspective projection},
  author = {Kooima, Robert},
  year   = {2009}
}

@article{1492264,
  author  = {Favalora, G.E.},
  journal = {Computer},
  title   = {Volumetric 3D displays and application infrastructure},
  year    = {2005},
  volume  = {38},
  number  = {8},
  pages   = {37-44},
  doi     = {10.1109/MC.2005.276}
}

@article{Gately:11,
  author    = {Matthew Gately and Yan Zhai and Mark Yeary and Erik Petrich and Lina Sawalha},
  journal   = {J. Display Technol.},
  keywords  = {LED displays; Spatial light modulators; Systems design; Three dimensional displays; Three dimensional imaging; Viewing angles},
  number    = {9},
  pages     = {503--514},
  publisher = {Optica Publishing Group},
  title     = {A Three-Dimensional Swept Volume Display Based on LED            Arrays},
  volume    = {7},
  month     = {Sep},
  year      = {2011},
  url       = {https://opg.optica.org/jdt/abstract.cfm?URI=jdt-7-9-503},
  abstract  = {Stereoscopic, or multi-view, display systems are considered            as better alternatives to conventional two-dimensional (2D)            displays, since such systems can provide important visual cues for            the human brain to process three-dimensional (3D) objects. An            auto-stereoscopic display is a device that can render 3D images for            viewers without the aid of special headgear or glasses. In this            paper, we present a new design of an auto-stereoscopic swept-volume            display (SVD) system based on light-emitting diode (LED) arrays.            This system is constituted of a display device and a graphics            control sub-system. The display device is a 2D rotating panel of            LEDs, relying on ``persistence of vision'' to generate 3D images. The            graphics control sub-system is composed of a combination of PC            software, field-programmable gate arrays (FPGAs), and supporting            circuitry. The primary task of the graphics control sub-system is to            process 3D data and control each LED. In addition, a new 3D image            generation and rendering method was developed to reduce the            bandwidth requirement and to facilitate 3D image display.            Demonstrated in the experiments, a prototype of this system is            capable of displaying 3D images and videos with full 360{\textdegree} view            angles.}
}


@patent{keane_volumetric_2016,
  title       = {Volumetric 3d display},
  url         = {https://patents.google.com/patent/WO2016092464A1/en},
  abstract    = {A drive system for a projection screen in a swept surface volumetric three dimensional (3D) display is disclosed where the drive system causes the projection screen to reciprocate through an excursion distance at a screen reciprocating frequency relative to a projection system. The drive system includes an actuator arrangement for generating an input reciprocating force substantially at the screen reciprocating frequency through an input excursion distance and a support structure for the projection screen. The support structure further includes a resonant mounting arrangement for the projection screen where the resonant mounting arrangement is operably connected to the actuator arrangement and configured to allow the projection screen to reciprocate through the excursion distance. In addition, the resonant mounting arrangement is configured to have a resonant frequency substantially equivalent to the screen reciprocating frequency on actuation of the actuator arrangement. A gaming console incorporating a swept surface volumetric 3D display based on the drive system is also disclosed.},
  nationality = {WO},
  assignee    = {Voxon, Co},
  number      = {WO2016092464A1},
  urldate     = {2024-01-17},
  author      = {KEANE, Sean Frederick and Jackson, Alan and SMITH, Gavin Finlay and TAMBLYN, William Joseph and SILVERMAN, Ken},
  month       = jun,
  year        = {2016},
  keywords    = {arrangement, drive system, projection screen, resonant, support component},
  file        = {Full Text PDF:/home/robbieb/Zotero/storage/NYB6ZYAG/KEANE et al. - 2016 - Volumetric 3d display.pdf:application/pdf}
}

@inproceedings{10.1117/12.480930,
  author       = {Gregg E. Favalora and Joshua Napoli and Deirdre M. Hall and Rick K. Dorval and Michael Giovinco and Michael J. Richmond and Won S. Chun},
  title        = {{100-million-voxel volumetric display}},
  volume       = {4712},
  booktitle    = {Cockpit Displays IX: Displays for Defense Applications},
  editor       = {Darrel G. Hopper},
  organization = {International Society for Optics and Photonics},
  publisher    = {SPIE},
  pages        = {300 -- 312},
  keywords     = {3-D display, volumetric, autostereoscopic, spatial light modulator, military visualization},
  year         = {2002},
  doi          = {10.1117/12.480930},
  url          = {https://doi.org/10.1117/12.480930}
}

@inproceedings{10.1145/1179849.1179982,
  author    = {Nayar, Shree K. and Anand, Vijay N.},
  title     = {3D volumetric display using passive optical scatterers},
  year      = {2006},
  isbn      = {1595933646},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1179849.1179982},
  doi       = {10.1145/1179849.1179982},
  booktitle = {ACM SIGGRAPH 2006 Sketches},
  pages     = {106–es},
  location  = {Boston, Massachusetts},
  series    = {SIGGRAPH '06}
}


@article{https://doi.org/10.1002/anie.202003160,
  author   = {Wan, Shigang and Zhou, Hongqi and Lin, Jinxiong and Lu, Wei},
  title    = {A Prototype of a Volumetric Three-Dimensional Display Based on Programmable Photo-Activated Phosphorescence},
  journal  = {Angewandte Chemie International Edition},
  volume   = {59},
  number   = {22},
  pages    = {8416-8420},
  keywords = {3D displays, phosphorescence, photo-activation, photochemistry, porphyrin},
  doi      = {https://doi.org/10.1002/anie.202003160},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202003160},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.202003160},
  abstract = {Abstract A proof-of-principle prototype of a volumetric 3D-displaying system is demonstrated by utilizing the photo-activated phosphorescence of two long-lived phosphorescent metal-porphyrins in dimethyl sulfoxide (DMSO), a photochemically deoxygenating solvent. The first phosphorescent sensitizer, Pt(TPBP), absorbs a light beam with a wavelength of 635 nm, and the sensitized singlet oxygen is scavenged by DMSO. The second phosphorescent emitter, Pt(OEP), absorbs a light beam with a wavelength of 532 nm and visibly phosphoresces only in the deoxygenated zone generated by the first sensitizer. The phosphorescent voxels, 3D images, and animations are well-defined by the intersections of the 635-nm and 532-nm light beams that are programmable by tuning of the excitation-power densities, the beam shapes, and the kinetics. As a pivotal selection rule for the phosphorescent molecular couple used in this 3D-displaying system, their absorptions and emissions must be orthogonal to each other, so that they can be excited and addressed independently.},
  year     = {2020}
}

@inproceedings{10.1145/2341931.2341937,
  author    = {Rowe, Anthony},
  title     = {Within an ocean of light: creating volumetric lightscapes},
  year      = {2012},
  isbn      = {9781450316750},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2341931.2341937},
  doi       = {10.1145/2341931.2341937},
  abstract  = {This paper documents explorations into an alternative platform for immersive and affective expression within spatial mixed reality installation experiences. It discusses and analyzes experiments that use an advanced LED cube to create immersive, interactive installations and environments where visitors and visuals share a common physical space. As a visual medium, the LED cube has very specific properties and affordances, and optimizing the potential for such systems to create meaningful experiences presents many interlinked challenges. Two artworks exploring these possibilities are discussed. Both have been exhibited internationally in a variety of settings. Together with this paper, the works shed some light on the design considerations and experiential possibilities afforded by LED cubes and arrays. They also suggest that LED grids have potential as an emerging medium for immersive volumetric visualizations that occupy physical space.},
  booktitle = {ACM SIGGRAPH 2012 Art Gallery},
  pages     = {358–365},
  numpages  = {8},
  location  = {Los Angeles, California},
  series    = {SIGGRAPH '12}
}


@misc{noauthor_products_nodate,
  title    = {Products},
  url      = {https://voxon.co/products/},
  language = {en-AU},
  urldate  = {2024-01-17},
  journal  = {Voxon Photonics},
  file     = {Snapshot:/home/robbieb/Zotero/storage/PNY3T7EQ/products.html:text/html}
}


@misc{voxon2,
  title    = {Voxon features on CNET’s What The Future},
  url      = {https://voxon.co/voxon-features-cnet-what-the-future/},
  language = {en-AU},
  urldate  = {2024-01-23},
  journal  = {Voxon Photonics},
  file     = {Snapshot:/home/robbieb/Zotero/storage/PNY3T7EQ/products.html:text/html}
}


@articleinfo{LAM2021050011,
  title   = {Holography, and the future of 3D display},
  journal = {Light: Advanced Manufacturing},
  volume  = {2},
  number  = {LAM2021050011},
  pages   = {446},
  year    = {2021},
  note    = {},
  issn    = {2689-9620},
  doi     = {10.37188/lam.2021.028},
  url     = {https://www.light-am.com//article/id/82c54cac-97b0-4d77-8ed8-4edda712fe7c},
  author  = {Pierre-Alexandre Blanche}
}

@inproceedings{10.1145/3290605.3300763,
  author    = {Fafard, Dylan and Stavness, Ian and Dechant, Martin and Mandryk, Regan and Zhou, Qian and Fels, Sidney},
  title     = {FTVR in VR: Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display},
  year      = {2019},
  isbn      = {9781450359702},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290605.3300763},
  doi       = {10.1145/3290605.3300763},
  abstract  = {Spherical fish tank virtual reality (FTVR) displays attempt to create a virtual "crystal ball" experience using head-tracked rendering. Almost all of these systems have omitted stereo cues, making them easy to build, but it is not clear how much this omission degrades the 3D experience. In this study, we evaluate performance and subjective effects of stereo on 3D perception and interaction tasks with a spherical FTVR display. To control for calibration error and tracking latency, we perform the evaluation on a simulated spherical display in VR. The results of our study provide a clear recommendation for the design and use of spherical FTVR displays: while omitting stereo may not be readily apparent for users, their performance will be significantly degraded (20\% - 91\% increase in median task time). Therefore, including stereo viewing in spherical displays is critical for use in FTVR.},
  booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–12},
  numpages  = {12},
  keywords  = {spherical display, fish tank virtual reality, 3d perception},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI '19}
}

@mastersthesis{Zabarauskas2012,
  author  = {Zabarauskas, Manfredas},
  title   = {3D Display Simulation Using Head-Tracking with Microsoft Kinect},
  school  = {University of Cambridge},
  year    = {2012},
  type    = {Unpublished master's thesis},
  address = {Wolfson College},
  month   = {May},
  note    = {Examination: Part II in Computer Science, June 2012. Word Count: 119761. Project Originator: M. Zabarauskas. Supervisor: Prof N. Dodgson}
}

@article{10.1063/1.5113467,
  author   = {Fushimi, Tatsuki and Marzo, Asier and Drinkwater, Bruce W. and Hill, Thomas L.},
  title    = {{Acoustophoretic volumetric displays using a fast-moving levitated particle}},
  journal  = {Applied Physics Letters},
  volume   = {115},
  number   = {6},
  pages    = {064101},
  year     = {2019},
  month    = {08},
  abstract = {{Displays have revolutionised the way we work and learn, and thus, the development of display technologies is of paramount importance. The possibility of a free-space display in which 3D graphics can be viewed from 360° without obstructions is an active area of research—holograms or lightfield displays can realize such a display, but they suffer from clipping and a limited field of view. Here, we use a phased array of ultrasonic emitters to realize a volumetric acoustophoretic display in which a millimetric particle is held in midair using acoustic radiation forces and moved rapidly along a 3D path. Synchronously, a light source illuminates the particle with the target colour at each 3D position. We show that it is possible to render simple figures in real time (10 frames per second) as well as raster images at a lower frame rate. Additionally, we explore the dynamics of a fast-moving particle inside a phased-array levitator and identify potential sources of degradation in image quality. The dynamics are nonlinear and lead to distortion in the displayed images, and this distortion increases with drawing speed. The created acoustophoretic display shows promise as a future form of display technology.}},
  issn     = {0003-6951},
  doi      = {10.1063/1.5113467},
  url      = {https://doi.org/10.1063/1.5113467},
  eprint   = {https://pubs.aip.org/aip/apl/article-pdf/doi/10.1063/1.5113467/13562800/064101\_1\_online.pdf}
}

@article{10.5555/1577069.1755843,
  author     = {King, Davis E.},
  title      = {Dlib-ml: A Machine Learning Toolkit},
  year       = {2009},
  issue_date = {12/1/2009},
  publisher  = {JMLR.org},
  volume     = {10},
  issn       = {1532-4435},
  abstract   = {There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.},
  journal    = {J. Mach. Learn. Res.},
  month      = {dec},
  pages      = {1755–1758},
  numpages   = {4}
}

@book{rost2009opengl,
  title     = {OpenGL shading language},
  author    = {Rost, Randi J and Licea-Kane, Bill and Ginsburg, Dan and Kessenich, John and Lichtenbelt, Barthold and Malan, Hugh and Weiblen, Mike},
  year      = {2009},
  publisher = {Pearson Education}
}

﻿@article{Hirayama2019,
  author   = {Hirayama, Ryuji
              and Martinez Plasencia, Diego
              and Masuda, Nobuyuki
              and Subramanian, Sriram},
  title    = {A volumetric display for visual, tactile and audio presentation using acoustic trapping},
  journal  = {Nature},
  year     = {2019},
  month    = {Nov},
  day      = {01},
  volume   = {575},
  number   = {7782},
  pages    = {320-323},
  abstract = {Science-fiction movies portray volumetric systems that provide not only visual but also tactile and audible three-dimensional (3D) content. Displays based on swept-volume surfaces1,2, holography3, optophoretics4, plasmonics5 or lenticular lenslets6 can create 3D visual content without the need for glasses or additional instrumentation. However, they are slow, have limited persistence-of-vision capabilities and, most importantly, rely on operating principles that cannot produce tactile and auditive content as well. Here we present the multimodal acoustic trap display (MATD): a levitating volumetric display that can simultaneously deliver visual, auditory and tactile content, using acoustophoresis as the single operating principle. Our system traps a particle acoustically and illuminates it with red, green and blue light to control its colour as it quickly scans the display volume. Using time multiplexing with a secondary trap, amplitude modulation and phase minimization, the MATD delivers simultaneous auditive and tactile content. The system demonstrates particle speeds of up to 8.75 metres per second and 3.75 metres per second in the vertical and horizontal directions, respectively, offering particle manipulation capabilities superior to those of other optical or acoustic approaches demonstrated until now. In addition, our technique offers opportunities for non-contact, high-speed manipulation of matter, with applications in computational fabrication7 and biomedicine8.},
  issn     = {1476-4687},
  doi      = {10.1038/s41586-019-1739-5},
  url      = {https://doi.org/10.1038/s41586-019-1739-5}
}

﻿@article{Hirayama2015,
  author   = {Hirayama, Ryuji
              and Naruse, Makoto
              and Nakayama, Hirotaka
              and Tate, Naoya
              and Shiraki, Atsushi
              and Kakue, Takashi
              and Shimobaba, Tomoyoshi
              and Ohtsu, Motoichi
              and Ito, Tomoyoshi},
  title    = {Design, Implementation and Characterization of a Quantum-Dot-Based Volumetric Display},
  journal  = {Scientific Reports},
  year     = {2015},
  month    = {Feb},
  day      = {16},
  volume   = {5},
  number   = {1},
  pages    = {8472},
  abstract = {In this study, we propose and experimentally demonstrate a volumetric display system based on quantum dots (QDs) embedded in a polymer substrate. Unlike conventional volumetric displays, our system does not require electrical wiring; thus, the heretofore unavoidable issue of occlusion is resolved because irradiation by external light supplies the energy to the light-emitting voxels formed by the QDs. By exploiting the intrinsic attributes of the QDs, the system offers ultrahigh definition and a wide range of colours for volumetric displays. In this paper, we discuss the design, implementation and characterization of the proposed volumetric display's first prototype. We developed an 8 {\texttimes} 8 {\texttimes} 8 display comprising two types of QDs. This display provides multi-colour three-type two-dimensional patterns when viewed from different angles. The QD-based volumetric display provides a new way to represent images and could be applied in leisure and advertising industries, among others.},
  issn     = {2045-2322},
  doi      = {10.1038/srep08472},
  url      = {https://doi.org/10.1038/srep08472}
}

﻿@article{Smalley2018,
  author   = {Smalley, D. E.
              and Nygaard, E.
              and Squire, K.
              and Van Wagoner, J.
              and Rasmussen, J.
              and Gneiting, S.
              and Qaderi, K.
              and Goodsell, J.
              and Rogers, W.
              and Lindsey, M.
              and Costner, K.
              and Monk, A.
              and Pearson, M.
              and Haymore, B.
              and Peatross, J.},
  title    = {A photophoretic-trap volumetric display},
  journal  = {Nature},
  year     = {2018},
  month    = {Jan},
  day      = {01},
  volume   = {553},
  number   = {7689},
  pages    = {486-490},
  abstract = {Photophoretic optical trapping of cellulose particles and persistence of vision are used to produce real-space volumetric images that can be viewed from all angles, in geometries unachievable by holograms and light-field technologies.},
  issn     = {1476-4687},
  doi      = {10.1038/nature25176},
  url      = {https://doi.org/10.1038/nature25176}
}

@book{hearn2004computer,
  title     = {Computer graphics with OpenGL},
  author    = {Hearn, Donald and Baker, M Pauline and Baker, M Pauline},
  volume    = {3},
  year      = {2004},
  publisher = {Pearson Prentice Hall Upper Saddle River, NJ:}
}

@inproceedings{4487481,
  author    = {Zwicker, Matthias and Yea, Sehoon and Vetro, Anthony and Forlines, Clifton and Matusik, Wojciech and Pfister, Hanspeter},
  booktitle = {2007 Conference Record of the Forty-First Asilomar Conference on Signals, Systems and Computers},
  title     = {Multi-view Video Compression for 3D Displays},
  year      = {2007},
  volume    = {},
  number    = {},
  pages     = {1506-1510},
  doi       = {10.1109/ACSSC.2007.4487481}
}

@inproceedings{10.1145/169059.169066,
  author    = {Ware, Colin and Arthur, Kevin and Booth, Kellogg S.},
  title     = {Fish tank virtual reality},
  year      = {1993},
  isbn      = {0897915755},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/169059.169066},
  doi       = {10.1145/169059.169066},
  abstract  = {The defining characteristics of what we call “Fish Tank Virtual Reality” are a stereo image of a three dimensional (3D) scene viewed on a monitor using a perspective projection coupled to the head position of the observer. We discuss some of the relative merits of this mode of viewing as compared to head mounted stereo displays. In addition, we report the experimental investigation of the following variables: 1) whether or not the perspective view is coupled to the actual viewpoint of the observer, 2) whether stereopsis is employed. Experiment 1 involved the subjective comparison of pairs of viewing conditions and the results suggest that head coupling may be more important than stereo in yielding a strong impression of three dimensionality. Experiment 2 involved subjects  tracing a path from a leaf of a 3D tree to the correct root (there were two trees intermeshed). The error rates ranged from 22\% in the pictorial display, to 1.3\% in the head coupled stereo display. The error rates for stereo alone and head coupling alone were 14.7\% and 3.2\% respectively. We conclude that head coupling is probably more important than stereo in 3D visualization and that head coupling and stereo combined provide an important enhancement to monitor based computer graphics.},
  booktitle = {Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems},
  pages     = {37–42},
  numpages  = {6},
  keywords  = {head coupled displays, scientific visualization, stereopsis, virtual reality},
  location  = {Amsterdam, The Netherlands},
  series    = {CHI '93}
}

      
@inproceedings{10.1145/3281505.3281540,
  author    = {Fafard, Dylan Brodie and Zhou, Qian and Chamberlain, Chris and Hagemann, Georg and Fels, Sidney and Stavness, Ian},
  title     = {Design and implementation of a multi-person fish-tank virtual reality display},
  year      = {2018},
  isbn      = {9781450360869},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3281505.3281540},
  doi       = {10.1145/3281505.3281540},
  abstract  = {A mixed reality experience with a physical display, that situates 3D virtual content within the real world, has the potential to help people work and play with 3D information. However, almost all of such "fish tank virtual reality" (FTVR) systems have been isolated to a single-person experience, making them unsuitable for collaborative tasks. In this paper, we present a display system that allows two people to have unobstructed 3D perspective views into a spherical display while still being able to see and talk to one another. We evaluated the system through qualitative observation at a four-day exhibition and found it was effective for providing a convincing, shared 3D experience.},
  booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
  articleno = {5},
  numpages  = {9},
  keywords  = {3D displays, co-location, collaboration, fish tank virtual reality, spherical displays, stereo},
  location  = {Tokyo, Japan},
  series    = {VRST '18}
}


@misc{brightvox_2023,
	title = {brightvox {3D} {June} , 2023  {NEXMEDIA} exhibition - {Holographic} {Signage} \#volumetric},
	url = {https://www.youtube.com/watch?v=mxyw6LkAtiQ},
	abstract = {brightvox 3D 　ホログラフィックサイネージ　
イベント向けにサイネージレンタルを行っています。

オフィシャルサイト：https://brightvox.jp/

3D hologram Signage
Volumetric Display

CC Attribution Contents
n- : Crystal Jellyfish (Leptomedusae)  CC-AT
Tasha.Lime : GoldFish
smice : Organic Voroni Ball},
	urldate = {2024-01-23},
	month = jun,
	year = {2023},
}

@ARTICLE{5701756,

  author={Urey, Hakan and Chellappan, Kishore V. and Erden, Erdem and Surman, Phil},

  journal={Proceedings of the IEEE}, 

  title={State of the Art in Stereoscopic and Autostereoscopic Displays}, 

  year={2011},

  volume={99},

  number={4},

  pages={540-555},

  doi={10.1109/JPROC.2010.2098351}}

@Article{Patel2017,
author={Patel, Shreya K.
and Cao, Jian
and Lippert, Alexander R.},
title={A volumetric three-dimensional digital light photoactivatable dye display},
journal={Nature Communications},
year={2017},
month={Jul},
day={11},
volume={8},
number={1},
pages={15239},
abstract={Volumetric three-dimensional displays offer spatially accurate representations of images with a 360{\textdegree} view, but have been difficult to implement due to complex fabrication requirements. Herein, a chemically enabled volumetric 3D digital light photoactivatable dye display (3D Light PAD) is reported. The operating principle relies on photoactivatable dyes that become reversibly fluorescent upon illumination with ultraviolet light. Proper tuning of kinetics and emission wavelengths enables the generation of a spatial pattern of fluorescent emission at the intersection of two structured light beams. A first-generation 3D Light PAD was fabricated using the photoactivatable dye N-phenyl spirolactam rhodamine B, a commercial picoprojector, an ultraviolet projector and a custom quartz imaging chamber. The system displays a minimum voxel size of 0.68{\thinspace}mm3, 200{\thinspace}$\mu$m resolution and good stability over repeated `on-off' cycles. A range of high-resolution 3D images and animations can be projected, setting the foundation for widely accessible volumetric 3D displays.},
issn={2041-1723},
doi={10.1038/ncomms15239},
url={https://doi.org/10.1038/ncomms15239}
}


@INPROCEEDINGS{4541126,
  author={Luebke, David},
  booktitle={2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro}, 
  title={CUDA: Scalable parallel programming for high-performance scientific computing}, 
  year={2008},
  volume={},
  number={},
  pages={836-838},
  keywords={Parallel programming;Scientific computing;Computer architecture;Biomedical computing;Computer graphics;Workstations;Multicore processing;Central Processing Unit;Marine vehicles;Yarn},
  doi={10.1109/ISBI.2008.4541126}}

@Inbook{Wang2014,
author="Wang, Endong
and Zhang, Qing
and Shen, Bo
and Zhang, Guangyong
and Lu, Xiaowei
and Wu, Qing
and Wang, Yajuan",
title="Intel Math Kernel Library",
bookTitle="High-Performance Computing on the Intel® Xeon Phi{\texttrademark}: How to Fully Exploit MIC Architectures",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="167--188",
abstract="In order to achieve optimal performance on multi-core and multi-processor systems, we need to fully use the features of parallelism and manage the memory hierarchical characters efficiently. The performance of sequential codes relies on the instruction-level and register-level SIMD parallelism, and also on high-speed cache-blocking functions. Threading applications need advanced planning to achieve satisfactory load balancing.",
isbn="978-3-319-06486-4",
doi="10.1007/978-3-319-06486-4_7",
url="https://doi.org/10.1007/978-3-319-06486-4_7"
}


@software{noauthor_microsoftazure-kinect-sensor-sdk_2024,
	title = {microsoft/Azure-Kinect-Sensor-{SDK}},
	rights = {{MIT}},
	url = {https://github.com/microsoft/Azure-Kinect-Sensor-SDK},
	abstract = {A cross platform (Linux and Windows) user mode {SDK} to read data from your Azure Kinect device.},
	publisher = {Microsoft},
	urldate = {2024-06-10},
	date = {2024-06-08},
	note = {original-date: 2018-12-10T17:50:05Z},
	keywords = {kinect, sdk},
}

#@book{woo1999opengl,
  title={OpenGL programming guide: the official guide to learning OpenGL, version 1.2},
  author={Woo, Mason and Neider, Jackie and Davis, Tom and Shreiner, Dave},
  year={1999},
  publisher={Addison-Wesley Longman Publishing Co., Inc.}
}


@software{herberth_dav1ddeglad_2024,
	title = {Dav1dde/glad},
	url = {https://github.com/Dav1dde/glad},
	abstract = {Multi-Language Vulkan/{GL}/{GLES}/{EGL}/{GLX}/{WGL} Loader-Generator based on the official specs.},
	author = {Herberth, David},
	urldate = {2024-06-10},
	date = {2024-06-10},
	note = {original-date: 2013-07-29T10:54:13Z},
	keywords = {c, code-generation, cpp, d, egl, generator, gl, glad, gles, gles2, glx, loader-generator, nim, opengl, pascal, python, rust, vulkan, wgl},
}


@software{noauthor_glfwglfw_2024,
	title = {glfw/glfw},
	rights = {Zlib},
	url = {https://github.com/glfw/glfw},
	abstract = {A multi-platform library for {OpenGL}, {OpenGL} {ES}, Vulkan, window and input},
	publisher = {{GLFW}},
	urldate = {2024-06-10},
	date = {2024-06-10},
	note = {original-date: 2013-04-18T15:24:53Z},
	keywords = {c, linux, macos, opengl, opengl-es, unix, vulkan, windows},
}

@Article{dlib09,
  author = {Davis E. King},
  title = {Dlib-ml: A Machine Learning Toolkit},
  journal = {Journal of Machine Learning Research},
  year = {2009},
  volume = {10},
  pages = {1755-1758},
}

@misc{lugaresi2019mediapipe,
      title={MediaPipe: A Framework for Building Perception Pipelines}, 
      author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
      year={2019},
      eprint={1906.08172},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@online{noauthor_microsofts_nodate,
	title = {Microsoft’s Azure Kinect Developer Kit Technology Transfers to Partner Ecosystem},
	url = {https://techcommunity.microsoft.com/t5/mixed-reality-blog/microsoft-s-azure-kinect-developer-kit-technology-transfers-to/ba-p/3899122},
	abstract = {Microsoft has ended production of Azure Kinect Developer Kit. The technology will continue to be available through the partner ecosystem. Microsoft has been a..},
	titleaddon = {{TECHCOMMUNITY}.{MICROSOFT}.{COM}},
	urldate = {2024-06-10},
	langid = {english},
}


@software{noauthor_bazelbuildbazel_2024,
	title = {bazelbuild/bazel},
	rights = {Apache-2.0},
	url = {https://github.com/bazelbuild/bazel},
	abstract = {a fast, scalable, multi-language and extensible build system},
	publisher = {Bazel},
	urldate = {2024-06-10},
	date = {2024-06-10},
	note = {original-date: 2014-06-12T16:00:38Z},
	keywords = {bazel, build, build-system, correct, fast, multi-language, scalable, test},
}

@online{McGuire2017Data,
  title = {Computer Graphics Archive},
  author = {Morgan McGuire},
  year = {2017},
  month = {July},
  url = {https://casual-effects.com/data}
}

@software{noauthor_tinyobjloadertinyobjloader_2024,
	title = {tinyobjloader/tinyobjloader},
	url = {https://github.com/tinyobjloader/tinyobjloader},
	abstract = {Tiny but powerful single file wavefront obj loader},
	publisher = {tinyobjloader},
	urldate = {2024-06-10},
	date = {2024-06-08},
	note = {original-date: 2012-08-15T02:44:30Z},
	keywords = {3d, c-plus-plus, cpp, loader, wavefront},
}

@software{noauthor_g-trucglm_2024,
	title = {g-truc/glm},
	url = {https://github.com/g-truc/glm},
	abstract = {{OpenGL} Mathematics ({GLM})},
	publisher = {G-Truc Creation},
	urldate = {2024-06-10},
	date = {2024-06-10},
	note = {original-date: 2012-09-06T00:04:56Z},
	keywords = {cpp, cpp-library, glm, header-only, mathematics, matrix, opengl, quaternion, simd, sycl, vector, vulkan},
}

@ARTICLE{Kersten1997-xq,
  title    = "Moving cast shadows induce apparent motion in depth",
  author   = "Kersten, D and Mamassian, P and Knill, D C",
  abstract = "Phenomenally strong visual illusions are described in which the
              motion of an object's cast shadow determines the perceived 3-D
              trajectory of the object. Simply adjusting the motion of a shadow
              is sufficient to induce dramatically different apparent
              trajectories of the object casting the shadow. Psychophysical
              results obtained with the use of 3-D graphics are reported which
              show that: (i) the information provided by the motion of an
              object's shadow overrides other strong sources of information and
              perceptual biases, such as the assumption of constant object size
              and a general viewpoint; (ii) the natural constraint of shadow
              darkness plays a role in the interpretation of a moving image
              patch as a shadow, but under some conditions even unnatural light
              shadows can induce apparent motion in depth of an object; (iii)
              when shadow motion is caused by a moving light source, the visual
              system incorrectly interprets the shadow motion as consistent
              with a moving object, rather than a moving light source. The
              results support the hypothesis that the human visual system
              incorporates a stationary light-source constraint in the
              perceptual processing of spatial layout of scenes.",
  journal  = "Perception",
  volume   =  26,
  number   =  2,
  pages    = "171--192",
  year     =  1997,
  address  = "United States",
  language = "en"
}


@software{noauthor_assimpassimp_2024,
	title = {assimp/assimp},
	url = {https://github.com/assimp/assimp},
	abstract = {The official Open-Asset-Importer-Library Repository. Loads 40+ 3D-file-formats into one unified and clean data structure.},
	publisher = {Open Asset Import Library},
	urldate = {2024-06-10},
	date = {2024-06-10},
	note = {original-date: 2010-05-05T12:53:45Z},
	keywords = {3mf, android, asset-pipeline, assets, assimp, c-plus-plus, collada, dae, fbx, fbx-exporter, game-development, gamedev-tool, gamedevelopment, ifc, objective-c, patreon, python, stl},
}

@software{unrealengine,
  author = {{Epic Games}},
  title = {Unreal Engine},
  url = {https://www.unrealengine.com},
  date = {2024-06-10},
}


@online{noauthor_unity_nodate,
	title = {Unity Real-Time Development Platform {\textbar} 3D, 2D, {VR} \& {AR} Engine},
	url = {https://unity.com/},
	abstract = {Create and grow real-time 3D games, apps, and experiences for entertainment, film, automotive, architecture, and more. Get started with Unity today.},
	titleaddon = {Unity},
	urldate = {2024-06-10},
	langid = {english},
}

@article{10.1145/965141.563893,
author = {Blinn, James F.},
title = {Models of light reflection for computer synthesized pictures},
year = {1977},
issue_date = {Summer 1977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {0097-8930},
url = {https://doi.org/10.1145/965141.563893},
doi = {10.1145/965141.563893},
abstract = {In the production of computer generated pictures of three dimensional objects, one stage of the calculation is the determination of the intensity of a given object once its visibility has been established. This is typically done by modelling the surface as a perfect diffuser, sometimes with a specular component added for the simulation of hilights. This paper presents a more accurate function for the generation of hilights which is based on some experimental measurements of how light reflects from real surfaces. It differs from previous models in that the intensity of the hilight changes with the direction of the light source. Also the position and shape of the hilights is somewhat different from that generated by simpler models. Finally, the hilight function generates different results when simulating metallic vs. nonmetallic surfaces. Many of the effects so generated are somewhat subtle and are apparent only during movie sequences. Some representative still frames from such movies are included.},
journal = {SIGGRAPH Comput. Graph.},
month = {jul},
pages = {192–198},
numpages = {7},
keywords = {computer graphics, graphic display, hidden surface removal, shading}
}


@inproceedings{10.1145/563858.563893,
author = {Blinn, James F.},
title = {Models of light reflection for computer synthesized pictures},
year = {1977},
isbn = {9781450373555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/563858.563893},
doi = {10.1145/563858.563893},
abstract = {In the production of computer generated pictures of three dimensional objects, one stage of the calculation is the determination of the intensity of a given object once its visibility has been established. This is typically done by modelling the surface as a perfect diffuser, sometimes with a specular component added for the simulation of hilights. This paper presents a more accurate function for the generation of hilights which is based on some experimental measurements of how light reflects from real surfaces. It differs from previous models in that the intensity of the hilight changes with the direction of the light source. Also the position and shape of the hilights is somewhat different from that generated by simpler models. Finally, the hilight function generates different results when simulating metallic vs. nonmetallic surfaces. Many of the effects so generated are somewhat subtle and are apparent only during movie sequences. Some representative still frames from such movies are included.},
booktitle = {Proceedings of the 4th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {192–198},
numpages = {7},
keywords = {computer graphics, graphic display, hidden surface removal, shading},
location = {San Jose, California},
series = {SIGGRAPH '77}
}

@online{noauthor_buy_nodate,
	title = {Azure Kinect developer kit – Microsoft},
	url = {https://www.microsoft.com/en-gb/d/azure-kinect-dk/8pp5vxmd9nhq},
	abstract = {Azure Kinect {DK} is a developer kit that contains a best-in-class 1 {MP} depth camera, 360˚ microphone array, 12 {MP} {RGB} camera and orientation sensor for building advanced computer vision and speech models.},
	titleaddon = {Microsoft Store},
	urldate = {2024-06-11},
	langid = {british},
	file = {Snapshot:/home/robbieb/Zotero/storage/BC8T4I7E/8pp5vxmd9nhq.html:text/html},
}

@online{noauthor_use_nodate,
	title = {Use Azure Kinect Sensor {SDK} image transformations {\textbar} Microsoft Learn},
	url = {https://learn.microsoft.com/en-us/azure/kinect-dk/use-image-transformation},
	urldate = {2024-06-11},
}

@ARTICLE{Kersten1997-so,
  title    = "Moving cast shadows induce apparent motion in depth",
  author   = "Kersten, D and Mamassian, P and Knill, D C",
  abstract = "Phenomenally strong visual illusions are described in which the
              motion of an object's cast shadow determines the perceived 3-D
              trajectory of the object. Simply adjusting the motion of a shadow
              is sufficient to induce dramatically different apparent
              trajectories of the object casting the shadow. Psychophysical
              results obtained with the use of 3-D graphics are reported which
              show that: (i) the information provided by the motion of an
              object's shadow overrides other strong sources of information and
              perceptual biases, such as the assumption of constant object size
              and a general viewpoint; (ii) the natural constraint of shadow
              darkness plays a role in the interpretation of a moving image
              patch as a shadow, but under some conditions even unnatural light
              shadows can induce apparent motion in depth of an object; (iii)
              when shadow motion is caused by a moving light source, the visual
              system incorrectly interprets the shadow motion as consistent
              with a moving object, rather than a moving light source. The
              results support the hypothesis that the human visual system
              incorporates a stationary light-source constraint in the
              perceptual processing of spatial layout of scenes.",
  journal  = "Perception",
  volume   =  26,
  number   =  2,
  pages    = "171--192",
  year     =  1997,
  address  = "United States",
  language = "en"
}

@book{bradski2008learning,
  title={Learning OpenCV: Computer vision with the OpenCV library},
  author={Bradski, Gary and Kaehler, Adrian},
  year={2008},
  publisher={" O'Reilly Media, Inc."}
}

@misc{king2015maxmargin,
      title={Max-Margin Object Detection}, 
      author={Davis E. King},
      year={2015},
      eprint={1502.00046},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@online{noauthor_dlib_nodate,
	title = {dlib C++ Library: Easily Create High Quality Object Detectors with Deep Learning},
	url = {https://blog.dlib.net/2016/10/easily-create-high-quality-object.html},
	urldate = {2024-06-11},
	file = {dlib C++ Library\: Easily Create High Quality Object Detectors with Deep Learning:/home/robbieb/Zotero/storage/QSH5UKTP/easily-create-high-quality-object.html:text/html},
}

@article{DBLP:journals/corr/Schmidhuber14,
  author       = {J{\"{u}}rgen Schmidhuber},
  title        = {Deep Learning in Neural Networks: An Overview},
  journal      = {CoRR},
  volume       = {abs/1404.7828},
  year         = {2014},
  url          = {http://arxiv.org/abs/1404.7828},
  eprinttype    = {arXiv},
  eprint       = {1404.7828},
  timestamp    = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Schmidhuber14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@online{noauthor_index_nodate,
	title = {Dlib provided files},
	url = {http://dlib.net/files/},
	urldate = {2024-06-11},
	file = {Index of /files:/home/robbieb/Zotero/storage/5DBLD734/files.html:text/html},
}

@INPROCEEDINGS{6909637,
  author={Kazemi, Vahid and Sullivan, Josephine},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={One millisecond face alignment with an ensemble of regression trees}, 
  year={2014},
  volume={},
  number={},
  pages={1867-1874},
  keywords={Shape;Regression tree analysis;Face;Training;Boosting;Training data;Vectors;Face Alignment;Real-Time;Gradient Boosting;Decision Trees},
  doi={10.1109/CVPR.2014.241}}

  @INPROCEEDINGS{6755925,
  author={Sagonas, Christos and Tzimiropoulos, Georgios and Zafeiriou, Stefanos and Pantic, Maja},
  booktitle={2013 IEEE International Conference on Computer Vision Workshops}, 
  title={300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge}, 
  year={2013},
  volume={},
  number={},
  pages={397-403},
  keywords={Databases;Shape;Training;Testing;Conferences;Computer vision;Accuracy},
  doi={10.1109/ICCVW.2013.59}}

  @misc{zhang2020mediapipe,
      title={MediaPipe Hands: On-device Real-time Hand Tracking}, 
      author={Fan Zhang and Valentin Bazarevsky and Andrey Vakunov and Andrei Tkachenka and George Sung and Chuo-Ling Chang and Matthias Grundmann},
      year={2020},
      eprint={2006.10214},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{adelson1984pmi,
  added-at = {2011-09-19T12:12:54.000+0200},
  author = {Adelson, E. H. and Anderson, C. H. and Bergen, J. R. and Burt, P. J. and Ogden, J. M.},
  biburl = {https://www.bibsonomy.org/bibtex/259dfac6a273a879eb5c33f0f5b740980/sac},
  citeulike-article-id = {1622723},
  interhash = {1b86abb78a10e821d19471cbc87bbe0e},
  intrahash = {59dfac6a273a879eb5c33f0f5b740980},
  journal = {RCA Engineer},
  keywords = {deepzoom image ma10 processing pyramid},
  number = 6,
  pages = {33--41},
  posted-at = {2007-09-05 11:12:27},
  priority = {0},
  timestamp = {2011-09-19T12:12:54.000+0200},
  title = {{1984, Pyramid methods in image processing}},
  volume = 29,
  year = 1984
}


@online{noauthor_hand_nodate,
	title = {Hand landmarks detection guide {\textbar} Google {AI} Edge},
	url = {https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker},
	titleaddon = {Google for Developers},
	urldate = {2024-06-11},
	langid = {english},
	file = {Snapshot:/home/robbieb/Zotero/storage/GVUZQ9QP/hand_landmarker.html:text/html},
}


@software{noauthor_palletsclick_2024,
	title = {pallets/click},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/pallets/click},
	abstract = {Python composable command line interface toolkit},
	publisher = {Pallets},
	urldate = {2024-06-11},
	date = {2024-06-11},
	note = {original-date: 2014-04-24T09:52:19Z},
	keywords = {cli, click, pallets, python},
}

@misc{gholami2023autodepthnet,
      title={AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras}, 
      author={Peyman Gholami and Robert Xiao},
      year={2023},
      eprint={2305.14731},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{https://doi.org/10.1002/jsid.1104,
author = {Dossena, Federico and Trentini, Andrea},
title = {OpenLDAT—A system for the measurement of display latency metrics},
journal = {Journal of the Society for Information Display},
volume = {30},
number = {8},
pages = {599-608},
doi = {https://doi.org/10.1002/jsid.1104},
url = {https://sid.onlinelibrary.wiley.com/doi/abs/10.1002/jsid.1104},
eprint = {https://sid.onlinelibrary.wiley.com/doi/pdf/10.1002/jsid.1104},
abstract = {Abstract The OpenLDAT project (short for Open Latency Display and Analysis Tool) is a system composed of a self-buildable device and an open-licensed application to measure several display latency metrics. The most interesting metric is total system latency: the time between an action happening in the physical world, like a mouse being clicked, and the result being displayed on the screen, such as a muzzle flash from a weapon in a videogame. There is currently no similar device on the market, and this type of measurement is traditionally done manually using a modified mouse and a high speed camera, but OpenLDAT can measure it automatically using a built-in test, or interactively, allowing testing of virtually any game or application, potentially on a separate machine. In addition to system latency, OpenLDAT can also measure more traditional metrics, such as pixel response times.},
year = {2022}
}

@online{noauthor_latency_nodate,
	title = {Latency mitigation strategies (by John Carmack)},
	author={Cormack, John},
	url = {https://danluu.com/latency-mitigation/},
	urldate = {2024-06-12},
	file = {Latency mitigation strategies (by John Carmack):/home/robbieb/Zotero/storage/84E48B3M/latency-mitigation.html:text/html},
}


@online{noauthor_apple_2024,
	title = {Apple Vision Pro Benchmark Test 1: See-Through Latency, Photon-to-Photon},
	url = {https://www.optofidelity.com/insights/blogs/apple-vision-pro-benchmark-test-1-see-through-latency-photon-to-photon},
	shorttitle = {Apple Vision Pro Benchmark Test 1},
	abstract = {The first and most notable observation with the {OptoFidelity} {BUDDY} test system is the Apple Vision Pro’s extremely low latency of {\textasciitilde}11ms. The competitors' results in the range of 35-40ms represent a previously considered good standard level.  We can, therefore, verify that Apple’s claim of their groundbreaking reduction of see-through latency is true.},
	urldate = {2024-06-12},
	date = {2024-02-14},
	langid = {english},
	file = {Snapshot:/home/robbieb/Zotero/storage/DNLFVT2N/apple-vision-pro-benchmark-test-1-see-through-latency-photon-to-photon.html:text/html},
}

@inproceedings{sharp2015accurate,
author = {Sharp, Toby and Keskin, Cem and Robertson, Duncan and Taylor, Jonathan and Shotton, Jamie and Kim, David and Rhemann, Christoph and Leichter, Ido and Vinnikov, Alon and Wei, Yichen and Freedman, Daniel and Krupka, Eyal and Fitzgibbon, Andrew and Izadi, Shahram and Kohli, Pushmeet},
title = {Accurate, Robust, and Flexible Real-time Hand Tracking},
booktitle = {CHI '15 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
year = {2015},
month = {April},
abstract = {We present a new real-time hand tracking system based on a single depth camera. The system can accurately reconstruct complex hand poses across a variety of subjects. It also allows for robust tracking, rapidly recovering from any temporary failures. Most uniquely, our tracker is highly flexible, dramatically improving upon previous approaches which have focused on front-facing close-range scenarios. This flexibility opens up new possibilities for human-computer interaction with examples including tracking at distances from tens of centimeters through to several meters (for controlling the TV at a distance), supporting tracking using a moving depth camera (for mobile scenarios), and arbitrary camera placements (for VR headsets). These features are achieved through a new pipeline that combines a multi-layered discriminative reinitialization strategy for per-frame pose estimation, followed by a generative model-fitting stage. We provide extensive technical details and a detailed qualitative and quantitative analysis.},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/accurate-robust-and-flexible-real-time-hand-tracking/},
pages = {3633-3642},
isbn = {978-1-4503-3145-6},
edition = {CHI '15 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
note = {Best of CHI Honorable Mention Award},
}


@online{noauthor_chessset_nodate,
	title = {Chessset wooden chess board with pieces {\textbar} 3D model},
	url = {https://www.cgtrader.com/free-3d-models/sports/toy/chessset},
	abstract = {Model available for download in Autodesk {FBX} format. Visit {CGTrader} and browse more than 1 million 3D models, including 3D print and real-time assets},
	titleaddon = {{CGTrader}},
	urldate = {2024-06-12},
	langid = {english},
	file = {Snapshot:/home/robbieb/Zotero/storage/UR2LYWJY/chessset.html:text/html},
}


@online{bank_rcsb_nodate,
	title = {{RCSB} {PDB} - 8QBK: Retron-Eco1 filament with {ADP}-ribosylated Effector (local map with 1 segment)},
	url = {https://www.rcsb.org/structure/8QBK},
	shorttitle = {{RCSB} {PDB} - 8QBK},
	abstract = {Retron-Eco1 filament with {ADP}-ribosylated Effector (local map with 1 segment)},
	author = {Bank, {RCSB} Protein Data},
	urldate = {2024-06-12},
	langid = {american},
	file = {Snapshot:/home/robbieb/Zotero/storage/VXCJX73Y/8QBK.html:text/html},
}

﻿@Article{Dhaou2019,
author={Dhaou, Dorra
and Ben Jabra, Saoussen
and Zagrouba, Ezzeddine},
title={A Review on Anaglyph 3D Image and Video Watermarking},
journal={3D Research},
year={2019},
month={Mar},
day={22},
volume={10},
number={2},
pages={13},
abstract={Thanks to the rapid growth of internet and the advanced development of 3D technology, 3D images and videos are proliferated over the networks. However, this causes several insecurity problems, and protecting this type of media has become a main challenge for many researchers. 3D watermarking is considered as an efficient solution for 3D data protection. In fact, it consists in embedding a secret key into a 3D content to protect it and in trying to extract it after any attack applied on marked 3D data. Anaglyph is the most popular and economical method among different 3D visualization methods. For this reason, it has become used for many 3D applications. Hence, 3D anaglyph watermarking presents an important research area, and several techniques have been proposed in order to protect this type of media. In this survey paper, the existing anaglyph 3D images and videos watermarking techniques are discussed. This discussion shows that the anaglyph video watermarking field is still not mature and new techniques should be proposed to improve the invisibility/robustness trade-off. In addition, based on the study of anaglyph generation methods, it is concluded that signature can be embedded during the generation stage.},
issn={2092-6731},
doi={10.1007/s13319-019-0223-1},
url={https://doi.org/10.1007/s13319-019-0223-1}
}


@online{noauthor_httpsdeveloperapplecommetalmetal-shading-language-specificationpdf_nodate,
	title = {Metal Shading Language Specification},
	url = {https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf},
	urldate = {2024-06-14},
}

@article{article-3D,
author = {Kim, Byeong-Cheol},
year = {2013},
month = {08},
title = {A Study on the Technique of the 3D Stereoscopic Cinema},
volume = {16},
journal = {Journal of Korea Multimedia Society},
doi = {10.9717/kmms.2013.16.8.994}
}
 
@inproceedings{Two-Kinds,
author = {Guan, Dongdong and Yang, Chenglei and Sun, Weisi and Wei, Yuan and Gai, Wei and Bian, Yulong and Liu, Juan and Sun, Qianhui and Zhao, Siwei and Meng, Xiangxu},
title = {Two Kinds of Novel Multi-user Immersive Display Systems},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174173},
doi = {10.1145/3173574.3174173},
abstract = {Stereoscopic display is a standard display mode for virtual reality environments. Typical 3D projection provides only a single stereoscopic video stream; thus co-located users cannot correctly perceive the virtual scene based on their own position and view. Several works devoted to developing multi-user stereoscopic display, but the number of users is very limited or the technical implementation is complicated. In this paper we put forward two flexible and simple projection-based multi-user stereoscopic display systems. The first one, named TPA, is based on a triple-projector array and provides a 120Hz active stereo for three users. Two TPAs can be combined to form a six-user system. The second one, named DPA, is a dual-projector and easy-implemented system providing individual stereoscopic video stream for two to six users. Finally, a co-located multi-user virtual fireman simulation training system and a virtual tennis simulation system were created to verify the effectiveness of our systems.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1-9},
numpages = {9},
keywords = {virtual reality, multi-users stereoscopic display, co-located collaboration},
location = {, Montreal QC, Canada, },
series = {CHI '18}
}

@misc{keselman2017intel,
      title={Intel RealSense Stereoscopic Depth Cameras}, 
      author={Leonid Keselman and John Iselin Woodfill and Anders Grunnet-Jepsen and Achintya Bhowmik},
      year={2017},
      eprint={1705.05548},
      archivePrefix={arXiv}
}


@online{noauthor_femto_nodate,
	title = {Femto Bolt},
	url = {https://shop.orbbec3d.com/Femto-Bolt},
	abstract = {{DetailsFemto} Bolt is a compact, high-performance device with multi-mode Depth and {RGB} cameras and {USB}-C connection for power and data. Its versatility and price makes it attractive for {AI}-developers for 3D vision applications. The depth camera uses Microsoft’s industry-proven {ToF} technology and has identical operating modes and performance as the Microsoft Azure Kinect. Performance and Programmability High resolution and high accuracy across a wide range of operations. Wide Field of View covers large area. {HDR} (High Dynamic Range) function preserves details. Easy camera setup with a rich set of {APIs} for various applications.Ease of use for {AI} Developers and Commercial Applications Depth and {RGB} cameras and {IMU} in a single device. Combined data and power with {USB}-C 3.2 connection. High precision synchronization trigger control.},
	titleaddon = {Orbbec},
	urldate = {2024-06-14},
	langid = {american},
	file = {Snapshot:/home/robbieb/Zotero/storage/7Z7EQG45/Femto-Bolt.html:text/html},
}

@Legislation{EuropeanParliament2016a,
  date       = {2016-05-04},
  location   = {OJ L 119, 4.5.2016, p. 1--88},
  title      = {Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council}},
  url        = {https://data.europa.eu/eli/reg/2016/679/oj},
  titleaddon = {of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation})},
  abstract   = {The General Data Protection Regulation (2016/679, "GDPR") is a Regulation in European Union (EU) law on data protection and privacy in the EU and the European Economic Area (EEA).},
  author     = {{European Parliament} and {Council of the European Union}},
  keywords   = {access consumer data data-processing freedom gdpr information justice law personal privacy protection security verification},
  urldate    = {2023-04-13},
}

@online{participation_equality_nodate,
	title = {Equality Act 2010},
	url = {https://www.legislation.gov.uk/ukpga/2010/15},
	abstract = {An Act to make provision to require Ministers of the Crown and others when making strategic decisions about the exercise of their functions to have regard to the desirability of reducing socio-economic inequalities; to reform and harmonise equality law and restate the greater part of the enactments relating to discrimination and harassment related to certain personal characteristics; to enable certain employers to be required to publish information about the differences in pay between male and female employees; to prohibit victimisation in certain circumstances; to require the exercise of certain functions to be with regard to the need to eliminate discrimination and other prohibited conduct; to enable duties to be imposed in relation to the exercise of public procurement functions; to increase equality of opportunity; to amend the law relating to rights and responsibilities in family relationships; and for connected purposes.},
	type = {Text},
	author = {{UK Parliament}},
	urldate = {2024-06-14},
	note = {Publisher: Statute Law Database},
}

@ARTICLE{Gong2009-vc,
  title    = "Application of a {3D} volumetric display for radiation therapy
              treatment planning I: quality assurance procedures",
  author   = "Gong, Xing and Kirk, Mike and Zusag, Tom and Khelashvili, Gocha
              and Chu, James and Napoli, Josh and Stutsman, Sandy",
  abstract = "To design and implement a set of quality assurance tests for an
              innovative 3D volumetric display for radiation treatment planning
              applications. A genuine 3D display (Perspecta Spatial 3D,
              Actuality-Systems Inc., Bedford, MA) has been integrated with the
              Pinnacle TPS (Philips Medical Systems, Madison WI), for treatment
              planning. The Perspecta 3D display renders a 25 cm diameter
              volume that is viewable from any side, floating within a
              translucent dome. In addition to displaying all 3D data exported
              from Pinnacle, the system provides a 3D mouse to define beam
              angles and apertures and to measure distance. The focus of this
              work is the design and implementation of a quality assurance
              program for 3D displays and specific 3D planning issues as guided
              by AAPM Task Group Report 53. A series of acceptance and quality
              assurance tests have been designed to evaluate the accuracy of CT
              images, contours, beams, and dose distributions as displayed on
              Perspecta. Three-dimensional matrices, rulers and phantoms with
              known spatial dimensions were used to check Perspecta's absolute
              spatial accuracy. In addition, a system of tests was designed to
              confirm Perspecta's ability to import and display Pinnacle data
              consistently. CT scans of phantoms were used to confirm beam
              field size, divergence, and gantry and couch angular accuracy as
              displayed on Perspecta. Beam angles were verified through
              Cartesian coordinate system measurements and by CT scans of
              phantoms rotated at known angles. Beams designed on Perspecta
              were exported to Pinnacle and checked for accuracy. Dose at
              sampled points were checked for consistency with Pinnacle and
              agreed within 1\% or 1 mm. All data exported from Pinnacle to
              Perspecta was displayed consistently. The 3D spatial display of
              images, contours, and dose distributions were consistent with
              Pinnacle display. When measured by the 3D ruler, the distances
              between any two points calculated using Perspecta agreed with
              Pinnacle within the measurement error.",
  journal  = "J Appl Clin Med Phys",
  volume   =  10,
  number   =  3,
  pages    = "96--114",
  month    =  jul,
  year     =  2009,
  address  = "United States",
  language = "en"
}

@article{radiation-therapy,
author = {Napoli, Joshua and Stutsman, Sandy and Chu, James and Gong, Xing and Rivard, Mark},
year = {2008},
month = {01},
pages = {},
title = {Radiation therapy planning using a volumetric 3-D display: PerspectaRAD}
}

@article{stickland_development_2003,
	title = {The development of a three dimensional imaging system and its application in computer aided design workstations},
	volume = {13},
	issn = {0957-4158},
	url = {https://www.sciencedirect.com/science/article/pii/S0957415801000526},
	doi = {https://doi.org/10.1016/S0957-4158(01)00052-6},
	abstract = {This paper details the application of a three dimensional imaging system known as planar contour imaging ({PCI}) to the presentation of images created by computer aided design ({CAD}) software. The three dimensional computational models were generated within a commercially available {CAD}/{CAM} software package from Delcam plc and then converted to stereo lithographic (.stl) format. The .stl file was then converted into a real, three dimensional, image by {PCI}. It was found that, in the same way that a user looks at a two dimensional image, the selection of the correct type and amount of data presented to the viewer was critical. However, when the image was refined, the three dimensional image was found to produce an impressive representation of the computational dataset.},
	pages = {521--532},
	number = {5},
	journaltitle = {Mechatronics},
	author = {Stickland, M. T. and {McKay}, S. and Scanlon, T. J.},
	date = {2003},
	keywords = {Computer aided design, Stereo lithography, Three dimensional imaging},
}

@inproceedings{10.1117/12.785009,
author = {Scott D. Robinson and Patrick J. Green},
title = {{3D display applications for defence and security}},
volume = {6956},
booktitle = {Display Technologies and Applications for Defense, Security, and Avionics II},
editor = {John Tudor Thomas and Andrew Malloy},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {69560B},
keywords = {3D displays, stereoscopic 3D displays, military 3D applications, geospatial analysis, medical 3D displays},
year = {2008},
doi = {10.1117/12.785009},
URL = {https://doi.org/10.1117/12.785009}
}

@online{noauthor_bae_nodate,
	title = {{BAE} Systems backs local tech company with global defence capability},
	url = {https://www.baesystems.com/en-aus/article/bae-systems-backs-local-tech-company},
	abstract = {World leading 3D volumetric display technology developed by South Australian company Voxon Photonics has won new work with {BAE} Systems’ {UK} based submarine business and the Australian based Hunter Class Frigate program.},
	titleaddon = {{BAE} Systems {\textbar} Australia},
	urldate = {2024-06-15},
	langid = {english},
	file = {Snapshot:/home/robbieb/Zotero/storage/HQ5KH2QU/bae-systems-backs-local-tech-company.html:text/html},
}


@online{noauthor_Voxon_community_nodate,
	title = {Voxon Photonics Media Kit},
	url = {https://voxon.co/community/},
	titleaddon = {Voxon Photonics},
	urldate = {2024-06-15},
	langid = {australian},
}

@inproceedings{10.1145/3214907.3214914,
author = {Zhou, Qian and Hagemann, Georg and Fels, Sidney and Fafard, Dylan and Wagemakers, Andrew and Chamberlain, Chris and Stavness, Ian},
title = {Coglobe: a co-located multi-person FTVR experience},
year = {2018},
isbn = {9781450358101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214907.3214914},
doi = {10.1145/3214907.3214914},
abstract = {Fish Tank Virtual Reality (FTVR) creates a compelling 3D illusion for a single person by rendering to their perspective with head-tracking. However, typically, other participants cannot share in the experience since they see a weirdly distorted image when they look at the FTVR display making it difficult to work and play together. To overcome this problem, we have created CoGlobe: a large spherical FTVR display for multiple users. Using CoGlobe, Siggraph attendees will experience the latest advance of FTVR that supports multiple people co-located in a shared space working and playing together through two different multiplayer games and tasks. We have created a competitive two-person 3D Pong game (Figure 1b) for attendees to experience a highly interactive two-person game looking at the CoGlobe. Onlookers can also watch using a variation of mixed reality with a tracked mobile smartphone. Using a smartphone as a second screen registered to the same virtual world enables multiple people to interact together as well. We have also created a cooperative multi-person 3D drone game (Figure 1c) to illustrate cooperation in FTVR. Attendees will also see how effective co-located 3D FTVR is when cooperating on a complex 3D mental rotation (Figure 1d) and a path-tracing task (Figure 1a). CoGlobe overcomes the limited situation awareness of headset VR, while retaining the benefits of cooperative 3D interaction and thus is an exciting direction for the next wave of 3D displays for work and fun for Siggraph attendees to experience.},
booktitle = {ACM SIGGRAPH 2018 Emerging Technologies},
articleno = {5},
numpages = {2},
keywords = {3D displays, collaboration, fish tank virtual reality, mixed reality},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@INPROCEEDINGS{7892376,
  author={Zhou, Qian and Wu, Kai and Miller, Gregor and Stavness, Ian and Fels, Sidney},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={3DPS: An auto-calibrated three-dimensional perspective-corrected spherical display}, 
  year={2017},
  volume={},
  number={},
  pages={455-456},
  keywords={Three-dimensional displays;Calibration;Visualization;Cameras;Rendering (computer graphics);Fish;Virtual reality;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892376}}

@inproceedings{10.1145/3084822.3091104,
author = {Belloc, O. R. and Nagamura, M. R. and Fonseca, D. and Rodrigues, A. and Souza, D. A. R. and Kurashima, C. S. and Almeida, M. M. and Borba, E. Z. and Lopes, R. D. and Zuffo, M. K.},
title = {OrbeVR: a handheld convex spherical virtual reality display},
year = {2017},
isbn = {9781450350129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084822.3091104},
doi = {10.1145/3084822.3091104},
abstract = {We present OrbeVR, a handheld concave spherical perspective-corrected display. OrbeVR displays combined images projected by multiple calibrated high-performance laser pico-projectors positioned inside a translucent sphere. Users position and OrbeVR are tracked, so the spherical display renders head-coupled perspectives with stereoscopic depth cues. OrbeVR is an extremely compact, lightweight and small Virtual Reality spherical display based on multiprojection technology. This emerging Virtual Reality technology enables exciting interactive display devices comparable to snow-globes.},
booktitle = {ACM SIGGRAPH 2017 Emerging Technologies},
articleno = {19},
numpages = {2},
keywords = {handheld display, spherical display, virtual reality},
location = {Los Angeles, California},
series = {SIGGRAPH '17}
}

@article{article-spheree,
author = {Ferreira, F. and Cabral, M. and Belloc, O. and Miller, G. and Kurashima, Celso and de Deus Lopes, Roseli and Stavness, Ian and Anacleto, Junia and Zuffo, Marcelo and Fels, S.},
year = {2014},
month = {07},
pages = {},
title = {Spheree: A 3D perspective-corrected interactive spherical scalable display},
isbn = {978-1-4503-2961-3},
doi = {10.1145/2614066.2614091}
}

@inproceedings{10.1145/1753326.1753535,
author = {Stavness, Ian and Lam, Billy and Fels, Sidney},
title = {pCubee: a perspective-corrected handheld cubic display},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753535},
doi = {10.1145/1753326.1753535},
abstract = {In this paper, we describe the design of a personal cubic display that offers novel interaction techniques for static and dynamic 3D content. We extended one-screen Fish Tank VR by arranging five small LCD panels into a box shape that is light and compact enough to be handheld. The display uses head-coupled perspective rendering and a real-time physics simulation engine to establish an interaction metaphor of having real objects inside a physical box that a user can hold and manipulate. We evaluated our prototype as a visualization tool and as an input device by comparing it with a conventional LCD display and mouse for a 3D tree-tracing task. We found that bimanual interaction with pCubee and a mouse offered the best performance and was most preferred by users. pCubee has potential in 3D visualization and interactive applications such as games, storytelling and education, as well as viewing 3D maps, medical and architectural data.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1381–1390},
numpages = {10},
keywords = {3d visualization, fish tank vr, handheld device, multi-screen display, physical interaction, user evaluation, user interface},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@article{article_cubee,
author = {Stavness, Ian and Vogt, Florian},
year = {2006},
month = {01},
pages = {},
title = {Cubee: A Cubic 3D Display for Physics-based Interaction},
doi = {10.1145/1179849.1180055}
}

@inproceedings{10.1145/3025453.3025806,
author = {Berard, Francois and Louis, Thibault},
title = {The Object Inside: Assessing 3D Examination with a Spherical Handheld Perspective-Corrected Display},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025806},
doi = {10.1145/3025453.3025806},
abstract = {Handheld Perspective Corrected Displays (HPCDs) can create the feeling of holding a virtual 3D object. They offer a direct interaction that is isomorphic to the manipulation of physical objects. This illusion depends on the ability to provide a natural visuomotor coupling. High performances systems are thus required to evaluate the fundamental merits of HPCDs. We built a spherical HPCD using external projection. The system offers a lightweight wireless seamless display with head-coupled stereo, robust tracking, and low latency. We compared users' performances with this HPCD and two other interactions that used a fixed planar display and either a touchpad or the spherical display as an indirect input. The task involved the inspection of complex virtual 3D puzzles. Physical puzzles were also tested as references. Contrary to expectations, all virtual interactions were found to be more efficient than a more "natural" physical puzzle. The HPCD yielded lower performances than the touchpad. This study indicates that the object examination task did not benefit from the accurate and precise rotations offered by the HPCD, but benefited from the high C/D gain of the touchpad.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4396–4404},
numpages = {9},
keywords = {3d display, depth perception, evaluation, handheld perspective corrected display (hpcd), isomorphic rotation, object examination},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2207676.2208640,
author = {Kim, Kibum and Bolton, John and Girouard, Audrey and Cooperstock, Jeremy and Vertegaal, Roel},
title = {TeleHuman: effects of 3d perspective on gaze and pose estimation with a life-size cylindrical telepresence pod},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208640},
doi = {10.1145/2207676.2208640},
abstract = {In this paper, we present TeleHuman, a cylindrical 3D display portal for life-size human telepresence. The TeleHuman 3D videoconferencing system supports 360 degree motion parallax as the viewer moves around the cylinder and optionally, stereoscopic 3D display of the remote person. We evaluated the effect of perspective cues on the conveyance of nonverbal cues in two experiments using a one-way telecommunication version of the system. The first experiment focused on how well the system preserves gaze and hand pointing cues. The second experiment evaluated how well the system conveys 3D body postural information. We compared 3 perspective conditions: a conventional 2D view, a 2D view with 360 degree motion parallax, and a stereoscopic view with 360 degree motion parallax. Results suggest the combined presence of motion parallax and stereoscopic cues significantly improved the accuracy with which participants were able to assess gaze and hand pointing cues, and to instruct others on 3D body poses. The inclusion of motion parallax and stereoscopic cues also led to significant increases in the sense of social presence and telepresence reported by participants.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2531–2540},
numpages = {10},
keywords = {videoconference, telepresence, organic user interfaces, motion parallax, cylindrical display, 3d video},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2207676.2208405,
author = {Hilliges, Otmar and Kim, David and Izadi, Shahram and Weiss, Malte and Wilson, Andrew},
title = {HoloDesk: direct 3d interactions with a situated see-through display},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208405},
doi = {10.1145/2207676.2208405},
abstract = {HoloDesk is an interactive system combining an optical see through display and Kinect camera to create the illusion that users are directly interacting with 3D graphics. A virtual image of a 3D scene is rendered through a half silvered mirror and spatially aligned with the real-world for the viewer. Users easily reach into an interaction volume displaying the virtual image. This allows the user to literally get their hands into the virtual display and to directly interact with an spatially aligned 3D virtual world, without the need for any specialised head-worn hardware or input device. We introduce a new technique for interpreting raw Kinect data to approximate and track rigid (e.g., books, cups) and non-rigid (e.g., hands, paper) physical objects and support a variety of physics-inspired interactions between virtual and real. In particular the algorithm models natural human grasping of virtual objects with more fidelity than previously demonstrated. A qualitative study highlights rich emergent 3D interactions, using hands and real-world objects. The implementation of HoloDesk is described in full, and example application scenarios explored. Finally, HoloDesk is quantitatively evaluated in a 3D target acquisition task, comparing the system with indirect and glasses-based variants.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2421-2430},
numpages = {10},
keywords = {see-through display, natural human grasping, kinect, augmented reality (ar), 3d physics interactions},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1109/3DUI.2008.4476617,
author = {Suenaga, T. and Tamai, Y. and Kurita, Y. and Matsumoto, Y. and Ogasawara, T.},
title = {Poster: Image-Based 3D Display with Motion Parallax using Face Tracking},
year = {2008},
isbn = {9781424420476},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/3DUI.2008.4476617},
doi = {10.1109/3DUI.2008.4476617},
abstract = {We propose an image-based 3D display with motion parallax using face tracking. Multi-view images of target objects are recorded in advance by utilizing a camera mounted on a 6 DOF manipulator. The 3D viewpoint of the user is measured by using a real-time non-contact face measurement system. One of the multi-view images is projected according as the camera position corresponding to the viewpoint of the user. The user can obtain 3D information of the object through a standard monitor by moving his/her head. A prototype system is developed and the consistency of the proposed method is confirmed by comparing generated images with captured images at the same viewpoints.},
booktitle = {Proceedings of the 2008 IEEE Symposium on 3D User Interfaces},
pages = {161-162},
numpages = {2},
keywords = {6 DOF manipulator, cameras, face tracking, image-based 3D display, motion parallax, real-time noncontact face measurement system},
series = {3DUI '08}
}

@INPROCEEDINGS{913797,
  author={Kunz, A.M. and Spagno, C.P.},
  booktitle={Proceedings IEEE Virtual Reality 2001}, 
  title={Modified shutter glasses for projection and picture acquisition in virtual environments}, 
  year={2001},
  volume={},
  number={},
  pages={281-282},
  keywords={Glass;Virtual reality;Lighting;Cameras;Collaboration;Computer displays;Layout;Intelligent networks;Product development;Collaborative tools},
  doi={10.1109/VR.2001.913797}}

@online{noauthor_crystal_nodate,
	title = {{CRYSTAL}: Spherical Fish Tank {VR} Display {\textbar} Human Communication Technologies Laboratory},
	url = {https://hct.ece.ubc.ca/project-3/},
	urldate = {2024-06-16},
	file = {CRYSTAL\: Spherical Fish Tank VR Display | Human Communication Technologies Laboratory:/home/robbieb/Zotero/storage/8ISVDKEQ/project-3.html:text/html},
}

@INPROCEEDINGS{7892378,
  author={Grubert, Jens and Kranz, Matthias},
  booktitle={2017 IEEE Virtual Reality (VR)}, 
  title={mpCubee: Towards a mobile perspective cubic display using mobile phones}, 
  year={2017},
  volume={},
  number={},
  pages={459-460},
  keywords={Three-dimensional displays;Mobile communication;Smart phones;Head;Rendering (computer graphics);Cameras;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems — Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2017.7892378}}


@online{noauthor_apple_vision_nodate,
	title = {Apple Vision Pro},
	url = {https://www.apple.com/uk/apple-vision-pro/},
	abstract = {Apple Vision Pro is Apple’s first spatial computer. It seamlessly blends digital content with your physical space using revolutionary technology.},
	titleaddon = {Apple (United Kingdom)},
	urldate = {2024-06-16},
	langid = {british},
}

﻿@Article{Huang2020,
	author={Huang, Yuge
	and Hsiang, En-Lin
	and Deng, Ming-Yang
	and Wu, Shin-Tson},
	title={Mini-LED, Micro-LED and OLED displays: present status and future perspectives},
	journal={Light: Science {\&} Applications},
	year={2020},
	month={Jun},
	day={18},
	volume={9},
	number={1},
	pages={105},
	abstract={Presently, liquid crystal displays (LCDs) and organic light-emitting diode (OLED) displays are two dominant flat panel display technologies. Recently, inorganic mini-LEDs (mLEDs) and micro-LEDs ($\mu$LEDs) have emerged by significantly enhancing the dynamic range of LCDs or as sunlight readable emissive displays. ``mLED, OLED, or $\mu$LED: who wins?'' is a heated debatable question. In this review, we conduct a comprehensive analysis on the material properties, device structures, and performance of mLED/$\mu$LED/OLED emissive displays and mLED backlit LCDs. We evaluate the power consumption and ambient contrast ratio of each display in depth and systematically compare the motion picture response time, dynamic range, and adaptability to flexible/transparent displays. The pros and cons of mLED, OLED, and $\mu$LED displays are analysed, and their future perspectives are discussed.},
	issn={2047-7538},
	doi={10.1038/s41377-020-0341-9},
	url={https://doi.org/10.1038/s41377-020-0341-9}
	}

@article{doi:10.1080/10447318.2020.1778351,
author = {Eunhee Chang, Hyun Taek Kim and Byounghyun Yoo},
title = {Virtual Reality Sickness: A Review of Causes and Measurements},
journal = {International Journal of Human–Computer Interaction},
volume = {36},
number = {17},
pages = {1658--1682},
year = {2020},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2020.1778351},
URL = { 
        https://doi.org/10.1080/10447318.2020.1778351
},
eprint = { 
        https://doi.org/10.1080/10447318.2020.1778351
}
}